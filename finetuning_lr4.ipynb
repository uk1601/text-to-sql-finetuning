{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Union\n",
    "# Removed argparse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig # If considering quantization later\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training # If using quantization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to output to stdout\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration settings for the fine-tuning script.\"\"\"\n",
    "    # --- Model and Tokenizer Arguments ---\n",
    "    model_name: str = \"google/gemma-3-1b-it\" # Hugging Face model identifier\n",
    "    hf_token: Optional[str] = None # Hugging Face API token (loads from .env if None)\n",
    "\n",
    "    # --- Dataset Arguments ---\n",
    "    dataset_name: str = \"gretelai/synthetic_text_to_sql\" # Hugging Face dataset identifier\n",
    "    train_subset_size: int = 5000 # Number of training examples (-1 for all)\n",
    "    val_subset_size: int = 750    # Number of validation examples (-1 for all)\n",
    "    test_subset_size: int = 1000   # Number of test examples (-1 for all)\n",
    "    validation_split_percentage: float = 0.15 # % of train for validation if needed\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    learning_rate: float = 2e-5  # Learning rate\n",
    "    num_train_epochs: int = 3      # Number of training epochs\n",
    "    per_device_train_batch_size: int = 1 # Batch size per device (training)\n",
    "    per_device_eval_batch_size: int = 1  # Batch size per device (evaluation)\n",
    "    gradient_accumulation_steps: int = 8 # Accumulate gradients over N steps\n",
    "    warmup_ratio: float = 0.1      # Warmup ratio for scheduler\n",
    "    weight_decay: float = 0.01     # Weight decay\n",
    "    max_grad_norm: float = 1.0     # Max gradient norm for clipping\n",
    "\n",
    "    # --- LoRA Configuration ---\n",
    "    lora_r: int = 4               # LoRA rank\n",
    "    lora_alpha: int = 16          # LoRA alpha scaling\n",
    "    lora_dropout: float = 0.05    # LoRA dropout\n",
    "    # Modules to target with LoRA (common for Gemma)\n",
    "    lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "    # --- Sequence Length Configuration ---\n",
    "    max_seq_length: int = 512     # Max sequence length for tokenization\n",
    "\n",
    "    # --- Environment and Reproducibility ---\n",
    "    seed: int = 42                # Random seed\n",
    "    use_mps_fallback: bool = True # Enable MPS fallback (set to False if not needed)\n",
    "\n",
    "    # --- Output and Logging ---\n",
    "    # Suffix for output dir (e.g., \"_r8_lr2e-5\"). Useful for hyperparameter tuning runs.\n",
    "    output_dir_suffix: str = f\"_r{lora_r}_lr2e-5\"\n",
    "    logging_steps: int = 10       # Log every N steps\n",
    "    eval_steps: int = 25          # Evaluate every N steps\n",
    "    save_steps: int = 50          # Save checkpoint every N steps\n",
    "    save_total_limit: int = 1     # Max checkpoints to keep\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment(config: Config):\n",
    "    \"\"\"Set up seeds, device, and MPS fallback based on Config.\"\"\"\n",
    "    # Set seeds\n",
    "    random.seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    torch.manual_seed(config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "    # Set MPS fallback if requested\n",
    "    if config.use_mps_fallback:\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        logger.info(\"PYTORCH_ENABLE_MPS_FALLBACK enabled.\")\n",
    "        # Optional: Lower high watermark ratio for potentially better memory management on MPS\n",
    "        os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "    # Determine device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(\"Using MPS device.\")\n",
    "        # Test MPS device\n",
    "        try:\n",
    "            x = torch.ones(1, device=device)\n",
    "            logger.info(f\"MPS test successful: {x.cpu().item()}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"MPS device test failed: {e}. Check PyTorch installation for MPS support.\")\n",
    "            device = torch.device(\"cpu\") # Fallback to CPU\n",
    "            logger.info(\"Falling back to CPU.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU device.\")\n",
    "\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(config: Config):\n",
    "    \"\"\"Load dataset, create splits, and select subsets based on Config.\"\"\"\n",
    "    logger.info(f\"Loading dataset: {config.dataset_name}\")\n",
    "    dataset = load_dataset(config.dataset_name)\n",
    "\n",
    "    # Ensure 'train' split exists\n",
    "    if 'train' not in dataset:\n",
    "        logger.error(\"Fatal: 'train' split not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    train_val_dataset = dataset['train']\n",
    "\n",
    "    # Create validation split if it doesn't exist or if specified size requires it\n",
    "    if 'validation' not in dataset or config.val_subset_size != -1:\n",
    "        logger.info(f\"Creating validation split ({config.validation_split_percentage * 100}%) from training data...\")\n",
    "        split_result = train_val_dataset.train_test_split(\n",
    "            test_size=config.validation_split_percentage,\n",
    "            seed=config.seed,\n",
    "            shuffle=True\n",
    "        )\n",
    "        train_dataset_full = split_result['train']\n",
    "        val_dataset_full = split_result['test']\n",
    "        logger.info(f\"Full train size: {len(train_dataset_full)}, Full validation size: {len(val_dataset_full)}\")\n",
    "    else:\n",
    "        train_dataset_full = train_val_dataset\n",
    "        val_dataset_full = dataset['validation']\n",
    "        logger.info(f\"Using existing validation split. Full train size: {len(train_dataset_full)}, Full validation size: {len(val_dataset_full)}\")\n",
    "\n",
    "    # Handle test split\n",
    "    if 'test' in dataset:\n",
    "        test_dataset_full = dataset['test']\n",
    "        logger.info(f\"Full test size: {len(test_dataset_full)}\")\n",
    "    else:\n",
    "        test_dataset_full = None\n",
    "        logger.warning(\"No test split found in the dataset.\")\n",
    "\n",
    "    # Select subsets\n",
    "    def select_subset(dataset_full, subset_size, name):\n",
    "        if dataset_full is None:\n",
    "            return None\n",
    "        # Use full dataset if subset_size is -1 or greater/equal to full size\n",
    "        if subset_size == -1 or subset_size >= len(dataset_full):\n",
    "            logger.info(f\"Using full {name} dataset ({len(dataset_full)} examples).\")\n",
    "            return dataset_full\n",
    "        else:\n",
    "            logger.info(f\"Selecting subset of {subset_size} examples for {name} dataset.\")\n",
    "            # Ensure subset_size is not larger than the dataset\n",
    "            actual_size = min(subset_size, len(dataset_full))\n",
    "            # Select requires indices, shuffle first for random subset\n",
    "            return dataset_full.shuffle(seed=config.seed).select(range(actual_size))\n",
    "\n",
    "    train_dataset = select_subset(train_dataset_full, config.train_subset_size, \"train\")\n",
    "    val_dataset = select_subset(val_dataset_full, config.val_subset_size, \"validation\")\n",
    "    test_dataset = select_subset(test_dataset_full, config.test_subset_size, \"test\")\n",
    "\n",
    "    # Create a DatasetDict\n",
    "    processed_datasets = DatasetDict()\n",
    "    if train_dataset: processed_datasets['train'] = train_dataset\n",
    "    if val_dataset: processed_datasets['validation'] = val_dataset\n",
    "    if test_dataset: processed_datasets['test'] = test_dataset\n",
    "\n",
    "    logger.info(f\"Final dataset sizes: { {k: len(v) for k, v in processed_datasets.items()} }\")\n",
    "\n",
    "    # Clean up large unused datasets\n",
    "    del dataset, train_val_dataset, train_dataset_full, val_dataset_full, test_dataset_full\n",
    "    gc.collect()\n",
    "\n",
    "    return processed_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(dataset_dict, tokenizer, max_seq_length):\n",
    "    \"\"\"Apply chat template, tokenize, and mask labels.\"\"\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        logger.warning(f\"Tokenizer pad_token set to eos_token: {tokenizer.eos_token}\")\n",
    "\n",
    "    # Define the formatting function\n",
    "    def format_and_prepare(example):\n",
    "        # --- Construct the prompt using Gemma's chat template structure ---\n",
    "        prompt = example['sql_prompt']\n",
    "        context = example['sql_context']\n",
    "        sql_output = example['sql']\n",
    "        explanation = example.get('sql_explanation', '') # Use .get for safety\n",
    "        target_response = f\"SQL: {sql_output}\"\n",
    "        if explanation:\n",
    "            target_response += f\"\\nExplanation: {explanation}\"\n",
    "\n",
    "        user_message = f\"Generate the SQL query for the following request based on the provided context.\\n\\nRequest: {prompt}\\n\\nDatabase Context:\\n{context}\"\n",
    "\n",
    "        # Apply template manually for precise label masking\n",
    "        prompt_part = f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        # Ensure EOS token marks the end of generation for the model to learn\n",
    "        response_part = f\"{target_response}{tokenizer.eos_token}\"\n",
    "\n",
    "        # Tokenize prompt and response parts separately\n",
    "        # Not adding special tokens here as they are manually included in the template parts\n",
    "        prompt_tokens = tokenizer(prompt_part, add_special_tokens=False)\n",
    "        response_tokens = tokenizer(response_part, add_special_tokens=False)\n",
    "\n",
    "        # Combine for input_ids and attention_mask\n",
    "        input_ids = prompt_tokens['input_ids'] + response_tokens['input_ids']\n",
    "        attention_mask = [1] * len(input_ids) # Attention mask should be 1 for all actual tokens\n",
    "\n",
    "        # Create labels: copy input_ids, then mask the prompt part\n",
    "        labels = list(input_ids) # Make a mutable copy\n",
    "        prompt_len = len(prompt_tokens['input_ids'])\n",
    "        # Mask prompt tokens by setting their labels to -100\n",
    "        labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "        # --- Truncation ---\n",
    "        # Truncate from the right if combined length exceeds max_seq_length\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "            attention_mask = attention_mask[:max_seq_length]\n",
    "            labels = labels[:max_seq_length]\n",
    "            # Ensure the last token isn't a masked token if possible, although unlikely with right truncation\n",
    "            # if labels[-1] == -100: logger.warning(\"Truncated sequence ends with a masked token.\")\n",
    "\n",
    "\n",
    "        # Note: Padding is handled dynamically by the DataCollatorForSeq2Seq\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    # Apply the function to all splits in the DatasetDict\n",
    "    logger.info(\"Applying formatting and tokenization...\")\n",
    "    # Determine columns to remove dynamically from the first split found\n",
    "    first_split_key = next(iter(dataset_dict))\n",
    "    remove_cols = list(dataset_dict[first_split_key].column_names)\n",
    "    tokenized_datasets = dataset_dict.map(\n",
    "        format_and_prepare,\n",
    "        # Consider batched=True for potential speedup if memory allows and logic is adapted\n",
    "        # batched=True,\n",
    "        # batch_size=100, # Adjust batch size if using batched=True\n",
    "        remove_columns=remove_cols\n",
    "    )\n",
    "    logger.info(\"Tokenization complete.\")\n",
    "\n",
    "    # Log token length statistics after tokenization (optional but helpful)\n",
    "    for split, dataset in tokenized_datasets.items():\n",
    "        if len(dataset) > 0: # Check if dataset split is not empty\n",
    "            lengths = [len(x) for x in dataset['input_ids']]\n",
    "            logger.info(f\"{split.capitalize()} split token lengths: Min={np.min(lengths)}, Mean={np.mean(lengths):.2f}, Max={np.max(lengths)}, Median={np.median(lengths)}\")\n",
    "            # Check if max length was hit often\n",
    "            truncated_count = sum(1 for length in lengths if length >= max_seq_length)\n",
    "            if truncated_count > 0:\n",
    "                logger.warning(f\"{truncated_count}/{len(lengths)} examples in {split} split potentially truncated to max_seq_length {max_seq_length}.\")\n",
    "        else:\n",
    "            logger.info(f\"{split.capitalize()} split is empty.\")\n",
    "\n",
    "\n",
    "    return tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_lora(config: Config, device):\n",
    "    \"\"\"Load the base model and apply LoRA configuration based on Config.\"\"\"\n",
    "\n",
    "    # --- Model Justification ---\n",
    "    # Choosing google/gemma-3-1b-it because:\n",
    "    # 1. Size: 3.1B parameters is manageable for fine-tuning on consumer hardware (especially MPS/CPU with optimizations).\n",
    "    # 2. Instruction-Tuned: '-it' indicates it's fine-tuned for following instructions, which is suitable for Text-to-SQL generation.\n",
    "    # 3. Performance: Gemma models generally show strong performance for their size class.\n",
    "    # 4. Openness: Relatively open model allows for easier experimentation.\n",
    "    logger.info(f\"Loading base model: {config.model_name}\")\n",
    "\n",
    "    # Determine dtype based on device\n",
    "    model_dtype = torch.float16 if device.type == \"mps\" else torch.float32\n",
    "    if device.type == \"cuda\":\n",
    "        # Check CUDA capability for bfloat16\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            model_dtype = torch.bfloat16\n",
    "            logger.info(\"Using bfloat16 for CUDA.\")\n",
    "        else:\n",
    "            model_dtype = torch.float16\n",
    "            logger.info(\"Using float16 for CUDA (bfloat16 not supported).\")\n",
    "\n",
    "    model_load_params = {\n",
    "        \"torch_dtype\": model_dtype,\n",
    "        \"low_cpu_mem_usage\": True, # Helps when loading large models\n",
    "        \"attn_implementation\": \"eager\", # Use eager attention for Gemma3 compatibility, esp. on MPS/CPU\n",
    "        \"use_cache\": False, # Important: Disable use_cache for gradient checkpointing & training\n",
    "    }\n",
    "    if config.hf_token:\n",
    "        model_load_params[\"token\"] = config.hf_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        **model_load_params\n",
    "    )\n",
    "\n",
    "    # --- LoRA Configuration ---\n",
    "    logger.info(f\"Configuring LoRA with r={config.lora_r}, alpha={config.lora_alpha}\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\", # Typically set to 'none' for LoRA\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=config.lora_target_modules\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    try:\n",
    "        model = get_peft_model(model, lora_config)\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Error applying LoRA. Check target modules: {config.lora_target_modules}. Error: {e}\")\n",
    "        # Attempt to find modules if error suggests mismatch\n",
    "        logger.info(\"Available module names containing 'proj':\")\n",
    "        for name, module in model.named_modules():\n",
    "            if 'proj' in name.lower():\n",
    "                logger.info(f\"- {name}\")\n",
    "        raise e # Re-raise after logging suggestions\n",
    "\n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Enable gradient checkpointing *after* applying LoRA\n",
    "    # Use reentrant=False for newer PyTorch versions if compatible, potentially saves more memory\n",
    "    logger.info(\"Enabling gradient checkpointing...\")\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    # Move model to device (do this *after* PEFT application and checkpoint enabling)\n",
    "    logger.info(f\"Moving model to device: {device}\")\n",
    "    model = model.to(device)\n",
    "    logger.info(f\"Model loaded, LoRA applied, and moved to {device}.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: Config, device, tokenized_datasets, tokenizer):\n",
    "    \"\"\"Configure and run the training process based on Config.\"\"\"\n",
    "\n",
    "    # Load Model with LoRA\n",
    "    model = load_model_and_lora(config, device)\n",
    "\n",
    "    # Create timestamped output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Use a descriptive name including key hyperparams if suffix is provided\n",
    "    output_dir_name = f\"gemma3_finetuned_{timestamp}{config.output_dir_suffix}\"\n",
    "    model_output_dir = os.path.join(\"models\", output_dir_name) # Relative path\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    logger.info(f\"Model output directory: {model_output_dir}\")\n",
    "\n",
    "    # Setup logging to file within the output directory\n",
    "    log_file = os.path.join(model_output_dir, \"training.log\")\n",
    "    # Remove existing file handler if script is re-run in same session (e.g., notebook)\n",
    "    root_logger = logging.getLogger()\n",
    "    # Check for existing file handlers to avoid duplicates\n",
    "    if any(isinstance(h, logging.FileHandler) and h.baseFilename == log_file for h in root_logger.handlers):\n",
    "         logger.warning(f\"Log file handler for {log_file} already exists. Skipping add.\")\n",
    "    else:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n",
    "        root_logger.addHandler(file_handler) # Add handler to root logger\n",
    "        logger.info(f\"Logging detailed output to: {log_file}\")\n",
    "\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    logger.info(\"Configuring Training Arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        lr_scheduler_type=\"cosine\", # Common scheduler\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        weight_decay=config.weight_decay,\n",
    "        max_grad_norm=config.max_grad_norm, # Gradient clipping\n",
    "\n",
    "        logging_dir=os.path.join(model_output_dir, \"logs\"), # Logs subdirectory\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=config.logging_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config.save_steps,\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=config.eval_steps,\n",
    "        load_best_model_at_end=True, # Load the best model based on eval loss\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # Device and Performance settings\n",
    "        # fp16=False, # Explicitly disable fp16/bf16 for MPS/CPU safety\n",
    "        # bf16=False,\n",
    "        gradient_checkpointing=True, # Already enabled on model, but set here too\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False}, # Match model setting\n",
    "        # torch_compile=False, # Can cause issues on MPS/CPU\n",
    "        dataloader_num_workers=0, # Safest for MPS/CPU\n",
    "        dataloader_pin_memory=False, # Avoid pinning memory\n",
    "\n",
    "        report_to=\"none\", # Disable external reporting (wandb, tensorboard) unless configured\n",
    "    )\n",
    "\n",
    "    # --- Data Collator ---\n",
    "    # Handles padding dynamically per batch to the longest sequence in the batch\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model, # Not strictly needed unless using model-specific padding logic\n",
    "        label_pad_token_id=-100, # Ensure labels are padded with ignore index\n",
    "        pad_to_multiple_of=8 # Optional: May improve performance on some hardware\n",
    "    )\n",
    "\n",
    "    # --- Callbacks ---\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config.early_stopping_patience,\n",
    "        early_stopping_threshold=config.early_stopping_threshold\n",
    "    )\n",
    "\n",
    "    # --- Initialize Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    logger.info(\"Starting training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        logger.info(\"Training completed successfully.\")\n",
    "\n",
    "        # Log training metrics\n",
    "        metrics = train_result.metrics\n",
    "        # Calculate perplexity if loss is available\n",
    "        try:\n",
    "            perplexity = np.exp(metrics[\"train_loss\"])\n",
    "            metrics[\"train_perplexity\"] = perplexity\n",
    "        except KeyError:\n",
    "            logger.warning(\"Could not calculate train perplexity: 'train_loss' not found in metrics.\")\n",
    "        except OverflowError:\n",
    "            metrics[\"train_perplexity\"] = float(\"inf\")\n",
    "\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state() # Save final trainer state (optimizer, scheduler, etc.)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {str(e)}\", exc_info=True) # Log traceback\n",
    "        raise # Re-raise exception after logging\n",
    "\n",
    "    # --- Save Final Model ---\n",
    "    logger.info(\"Saving the best model...\")\n",
    "    # Trainer automatically saves the best checkpoint based on eval_loss\n",
    "    # The final model state loaded corresponds to the best checkpoint due to load_best_model_at_end=True\n",
    "    best_model_path = trainer.state.best_model_checkpoint\n",
    "    if best_model_path:\n",
    "        logger.info(f\"Best model checkpoint identified at: {best_model_path}\")\n",
    "        # You might want to save the final adapter separately for easier loading later\n",
    "        final_adapter_dir = os.path.join(model_output_dir, \"final_adapter\")\n",
    "        model.save_pretrained(final_adapter_dir) # Saves only the adapter weights\n",
    "        tokenizer.save_pretrained(final_adapter_dir) # Save tokenizer with adapter\n",
    "        logger.info(f\"Final adapter saved to: {final_adapter_dir}\")\n",
    "        # Optionally copy the full best checkpoint if needed\n",
    "        # final_model_dir = os.path.join(model_output_dir, \"best_checkpoint_full\")\n",
    "        # if os.path.exists(best_model_path):\n",
    "        #     shutil.copytree(best_model_path, final_model_dir, dirs_exist_ok=True)\n",
    "        #     logger.info(f\"Full best checkpoint copied to: {final_model_dir}\")\n",
    "\n",
    "    else:\n",
    "        # Should not happen with load_best_model_at_end=True unless training stopped very early\n",
    "        logger.warning(\"Best model checkpoint not found. Saving final model state as adapter.\")\n",
    "        final_adapter_dir = os.path.join(model_output_dir, \"final_adapter\")\n",
    "        model.save_pretrained(final_adapter_dir)\n",
    "        tokenizer.save_pretrained(final_adapter_dir)\n",
    "        logger.info(f\"Final adapter saved to: {final_adapter_dir}\")\n",
    "\n",
    "\n",
    "    # --- Evaluate on Test Set (if available) ---\n",
    "    if \"test\" in tokenized_datasets:\n",
    "        logger.info(\"Evaluating on the test set using the best model...\")\n",
    "        try:\n",
    "            test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "            # Calculate perplexity for test set\n",
    "            try:\n",
    "                test_perplexity = np.exp(test_results[\"eval_loss\"])\n",
    "                test_results[\"eval_perplexity\"] = test_perplexity\n",
    "            except KeyError:\n",
    "                 logger.warning(\"Could not calculate test perplexity: 'eval_loss' not found.\")\n",
    "            except OverflowError:\n",
    "                 test_results[\"eval_perplexity\"] = float(\"inf\")\n",
    "\n",
    "            logger.info(f\"Test Results: {test_results}\")\n",
    "            trainer.log_metrics(\"test\", test_results)\n",
    "            # Save test metrics in the main output directory\n",
    "            with open(os.path.join(model_output_dir, \"test_results.json\"), \"w\") as f:\n",
    "                json.dump(test_results, f, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during final test evaluation: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.info(\"No test set provided for final evaluation.\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    logger.info(\"Cleaning up memory...\")\n",
    "    # Ensure model and trainer are deleted\n",
    "    try:\n",
    "        del model\n",
    "    except NameError: pass\n",
    "    try:\n",
    "        del trainer\n",
    "    except NameError: pass\n",
    "    try:\n",
    "        del data_collator\n",
    "    except NameError: pass\n",
    "    gc.collect() # Run garbage collection\n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(\"Fine-tuning process finished.\")\n",
    "    logger.info(f\"Model adapter and logs saved in: {model_output_dir}\")\n",
    "    return model_output_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:17:34,038 - __main__ - INFO - Script configuration: {}\n",
      "2025-04-21 20:17:34,039 - __main__ - INFO - Loaded Hugging Face token from .env file.\n",
      "2025-04-21 20:17:34,042 - __main__ - INFO - PYTORCH_ENABLE_MPS_FALLBACK enabled.\n",
      "2025-04-21 20:17:34,055 - __main__ - INFO - Using MPS device.\n",
      "2025-04-21 20:17:34,073 - __main__ - INFO - MPS test successful: 1.0\n",
      "2025-04-21 20:17:34,073 - __main__ - INFO - Loading dataset: gretelai/synthetic_text_to_sql\n",
      "2025-04-21 20:17:35,481 - __main__ - INFO - Creating validation split (15.0%) from training data...\n",
      "2025-04-21 20:17:35,486 - __main__ - INFO - Full train size: 85000, Full validation size: 15000\n",
      "2025-04-21 20:17:35,486 - __main__ - INFO - Full test size: 5851\n",
      "2025-04-21 20:17:35,486 - __main__ - INFO - Selecting subset of 5000 examples for train dataset.\n",
      "2025-04-21 20:17:35,491 - __main__ - INFO - Selecting subset of 750 examples for validation dataset.\n",
      "2025-04-21 20:17:35,493 - __main__ - INFO - Selecting subset of 1000 examples for test dataset.\n",
      "2025-04-21 20:17:35,495 - __main__ - INFO - Final dataset sizes: {'train': 5000, 'validation': 750, 'test': 1000}\n",
      "2025-04-21 20:17:35,592 - __main__ - INFO - Loading tokenizer for google/gemma-3-1b-it...\n",
      "2025-04-21 20:17:36,466 - __main__ - INFO - Applying formatting and tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcb44ecc9614ecdbc2dd0026bf8889e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23818e0ff4f244cd95188325449744cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:17:37,703 - __main__ - INFO - Tokenization complete.\n",
      "2025-04-21 20:17:37,884 - __main__ - INFO - Train split token lengths: Min=72, Mean=229.57, Max=512, Median=218.0\n",
      "2025-04-21 20:17:37,884 - __main__ - WARNING - 10/5000 examples in train split potentially truncated to max_seq_length 512.\n",
      "2025-04-21 20:17:37,908 - __main__ - INFO - Validation split token lengths: Min=83, Mean=226.84, Max=492, Median=214.0\n",
      "2025-04-21 20:17:37,939 - __main__ - INFO - Test split token lengths: Min=82, Mean=225.20, Max=512, Median=212.5\n",
      "2025-04-21 20:17:37,939 - __main__ - WARNING - 1/1000 examples in test split potentially truncated to max_seq_length 512.\n",
      "2025-04-21 20:17:37,939 - __main__ - INFO - Loading base model: google/gemma-3-1b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:17:39,185 - __main__ - INFO - Configuring LoRA with r=4, alpha=16\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "2025-04-21 20:17:39,233 - __main__ - INFO - Enabling gradient checkpointing...\n",
      "2025-04-21 20:17:39,235 - __main__ - INFO - Moving model to device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udaykiran/Documents/NEU/Prompt Engineering/Finetuning/.venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:17:39,532 - __main__ - INFO - Model loaded, LoRA applied, and moved to mps.\n",
      "2025-04-21 20:17:39,532 - __main__ - INFO - Model output directory: models/gemma3_finetuned_20250421_201739_r4_lr2e-5\n",
      "2025-04-21 20:17:39,533 - __main__ - INFO - Logging detailed output to: models/gemma3_finetuned_20250421_201739_r4_lr2e-5/training.log\n",
      "2025-04-21 20:17:39,533 - __main__ - INFO - Configuring Training Arguments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zw/rm73pxq9065980p1jg6sh01r0000gn/T/ipykernel_55230/2914248655.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:17:39,543 - __main__ - INFO - Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 375/1875 1:02:10 < 4:10:01, 0.10 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.129100</td>\n",
       "      <td>2.175264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.868100</td>\n",
       "      <td>1.913415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.582100</td>\n",
       "      <td>1.572859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.228700</td>\n",
       "      <td>1.260759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.070500</td>\n",
       "      <td>1.047199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.906300</td>\n",
       "      <td>0.905775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.787441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.778900</td>\n",
       "      <td>0.678301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.629800</td>\n",
       "      <td>0.640971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.618557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.602516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.590915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>0.581916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>0.576466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.568309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 21:19:55,037 - __main__ - INFO - Training completed successfully.\n",
      "***** train metrics *****\n",
      "  epoch                    =        0.6\n",
      "  total_flos               =  2720063GF\n",
      "  train_loss               =     1.0082\n",
      "  train_perplexity         =     2.7408\n",
      "  train_runtime            = 1:02:15.39\n",
      "  train_samples_per_second =      4.016\n",
      "  train_steps_per_second   =      0.502\n",
      "2025-04-21 21:19:55,039 - __main__ - INFO - Saving the best model...\n",
      "2025-04-21 21:19:55,039 - __main__ - INFO - Best model checkpoint identified at: models/gemma3_finetuned_20250421_201739_r4_lr2e-5/checkpoint-350\n",
      "2025-04-21 21:19:55,855 - __main__ - INFO - Final adapter saved to: models/gemma3_finetuned_20250421_201739_r4_lr2e-5/final_adapter\n",
      "2025-04-21 21:19:55,856 - __main__ - INFO - Evaluating on the test set using the best model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 21:23:05,111 - __main__ - INFO - Test Results: {'eval_loss': 0.5779856443405151, 'eval_runtime': 189.2521, 'eval_samples_per_second': 5.284, 'eval_steps_per_second': 5.284, 'epoch': 0.6, 'eval_perplexity': np.float64(1.7824443352376447)}\n",
      "***** test metrics *****\n",
      "  epoch                   =        0.6\n",
      "  eval_loss               =      0.578\n",
      "  eval_perplexity         =     1.7824\n",
      "  eval_runtime            = 0:03:09.25\n",
      "  eval_samples_per_second =      5.284\n",
      "  eval_steps_per_second   =      5.284\n",
      "2025-04-21 21:23:05,113 - __main__ - INFO - Cleaning up memory...\n",
      "2025-04-21 21:23:05,958 - __main__ - INFO - Fine-tuning process finished.\n",
      "2025-04-21 21:23:05,958 - __main__ - INFO - Model adapter and logs saved in: models/gemma3_finetuned_20250421_201739_r4_lr2e-5\n",
      "2025-04-21 21:23:05,959 - __main__ - INFO - Training successful. Best model adapter saved in subdirectories within: models/gemma3_finetuned_20250421_201739_r4_lr2e-5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create config instance\n",
    "    config = Config()\n",
    "    logger.info(f\"Script configuration: {vars(config)}\") # Log the config used\n",
    "\n",
    "    # Load HF Token if not provided via config\n",
    "    if config.hf_token is None:\n",
    "        load_dotenv()\n",
    "        config.hf_token = os.getenv('HF_TOKEN')\n",
    "        if config.hf_token:\n",
    "            logger.info(\"Loaded Hugging Face token from .env file.\")\n",
    "\n",
    "    # Setup device and seeds\n",
    "    device = setup_environment(config)\n",
    "\n",
    "    # Load and prepare dataset\n",
    "    processed_datasets = load_and_prepare_dataset(config)\n",
    "\n",
    "    # Load tokenizer (needed for preprocessing)\n",
    "    logger.info(f\"Loading tokenizer for {config.model_name}...\")\n",
    "    tokenizer_load_params = {}\n",
    "    if config.hf_token: tokenizer_load_params['token'] = config.hf_token\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.model_name, **tokenizer_load_params)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal: Failed to load tokenizer '{config.model_name}'. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Preprocess and tokenize data\n",
    "    try:\n",
    "        tokenized_datasets = preprocess_and_tokenize(processed_datasets, tokenizer, config.max_seq_length)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal: Failed during data preprocessing/tokenization. Error: {e}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Check if datasets are empty after processing (important!)\n",
    "    if not tokenized_datasets or not tokenized_datasets.get(\"train\"):\n",
    "         logger.error(\"Fatal: Training dataset is empty after preprocessing. Check data loading and tokenization steps.\")\n",
    "         sys.exit(1)\n",
    "    if not tokenized_datasets.get(\"validation\"):\n",
    "         logger.error(\"Fatal: Validation dataset is empty after preprocessing. Check data loading and tokenization steps.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # Run training\n",
    "    try:\n",
    "        trained_model_path = train(config, device, tokenized_datasets, tokenizer)\n",
    "        logger.info(f\"Training successful. Best model adapter saved in subdirectories within: {trained_model_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Training failed.\", exc_info=True)\n",
    "        sys.exit(1) # Exit with error status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
