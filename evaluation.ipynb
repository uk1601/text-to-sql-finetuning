{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import shutil\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm for better display\n",
    "from typing import Dict, List, Optional, Any # Added Any\n",
    "\n",
    "# --- Imports for ipywidgets ---\n",
    "# Removed ipywidgets imports as they are not used in this script version\n",
    "# --- End ipywidgets imports ---\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig # If considering quantization\n",
    ")\n",
    "from peft import PeftModel\n",
    "import evaluate # Hugging Face Evaluate library\n",
    "import sqlparse # For SQL normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "# Reduce logging spam from underlying libraries\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "# Keep our own logger at INFO level\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalConfig:\n",
    "    \"\"\"Configuration settings for the evaluation script.\"\"\"\n",
    "    # --- Model Identification ---\n",
    "    base_model_name: str = \"google/gemma-3-1b-it\"\n",
    "    # List of adapter paths (relative to the script location)\n",
    "    # Add the paths to the 'final_adapter' directories from your runs\n",
    "    adapter_paths: Dict[str, str] = {\n",
    "        \"LoRA_r4_lr2e-5\": \"models/gemma3_finetuned_20250421_201739_r4_lr2e-5/final_adapter\",\n",
    "        \"LoRA_r8_lr2e-5\": \"models/gemma3_finetuned_20250421_212338_r8_lr2e-5/final_adapter\",\n",
    "        \"LoRA_r16_lr2e-5\": \"models/gemma3_finetuned_20250421_224200_r16_lr2e-5/final_adapter\",\n",
    "    }\n",
    "    # Optionally add more adapters if you run more experiments\n",
    "\n",
    "    # --- Dataset ---\n",
    "    dataset_name: str = \"gretelai/synthetic_text_to_sql\"\n",
    "    # Use the same test subset size as in training evaluation, or -1 for full test set\n",
    "    test_subset_size: int = 15 # Reduced for quick testing, set back to 30 or -1\n",
    "    seed: int = 42 # Seed for subset selection if used\n",
    "\n",
    "    # --- Evaluation Parameters ---\n",
    "    metrics_to_compute: List[str] = [\"bleu\", \"rouge\", \"exact_match\"]\n",
    "    # Generation config\n",
    "    max_new_tokens: int = 256 # Max tokens to generate for SQL + Explanation\n",
    "    temperature: float = 0.1 # Set a default positive temperature (will be ignored if do_sample=False)\n",
    "    do_sample: bool = False # Set to False for deterministic evaluation (greedy)\n",
    "    # Max length for tokenizer input (prompt part). Should generally match training.\n",
    "    # Input will be truncated to (max_seq_length - max_new_tokens)\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "    # --- Environment ---\n",
    "    use_mps_fallback: bool = True # Enable MPS fallback if needed\n",
    "    hf_token: Optional[str] = None # Loads from .env if None\n",
    "\n",
    "    # --- Output Directory ---\n",
    "    evaluation_output_dir: str = \"evaluation\" # Name of the subdirectory for outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_eval_environment(config: EvalConfig):\n",
    "    \"\"\"Set up seeds, device, and MPS fallback.\"\"\"\n",
    "    random.seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    torch.manual_seed(config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "    if config.use_mps_fallback:\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        logger.info(\"PYTORCH_ENABLE_MPS_FALLBACK enabled.\")\n",
    "        os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(\"Using MPS device.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU device.\")\n",
    "\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(config: EvalConfig):\n",
    "    \"\"\"Load and prepare the test dataset subset.\"\"\"\n",
    "    logger.info(f\"Loading dataset: {config.dataset_name}\")\n",
    "    dataset = load_dataset(config.dataset_name)\n",
    "\n",
    "    if 'test' not in dataset:\n",
    "        logger.warning(\"No 'test' split found. Using 'train' split for evaluation.\")\n",
    "        test_dataset_full = dataset['train']\n",
    "    else:\n",
    "        test_dataset_full = dataset['test']\n",
    "\n",
    "    logger.info(f\"Full test set size: {len(test_dataset_full)}\")\n",
    "\n",
    "    # Select subset\n",
    "    if config.test_subset_size == -1 or config.test_subset_size >= len(test_dataset_full):\n",
    "        logger.info(f\"Using full test dataset ({len(test_dataset_full)} examples).\")\n",
    "        test_dataset = test_dataset_full\n",
    "    else:\n",
    "        logger.info(f\"Selecting subset of {config.test_subset_size} examples for test dataset.\")\n",
    "        actual_size = min(config.test_subset_size, len(test_dataset_full))\n",
    "        test_dataset = test_dataset_full.shuffle(seed=config.seed).select(range(actual_size))\n",
    "\n",
    "    logger.info(f\"Using {len(test_dataset)} examples for evaluation.\")\n",
    "    return test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_for_generation(example, tokenizer):\n",
    "    \"\"\"Formats the prompt using the chat template for generation.\"\"\"\n",
    "    prompt = example['sql_prompt']\n",
    "    context = example['sql_context']\n",
    "\n",
    "    user_message = f\"Generate the SQL query for the following request based on the provided context.\\n\\nRequest: {prompt}\\n\\nDatabase Context:\\n{context}\"\n",
    "\n",
    "    # Use apply_chat_template for generation - it should add the prompt structure correctly\n",
    "    # including the final turn marker for the assistant.\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    try:\n",
    "        # Set add_generation_prompt=True to correctly format for prompting the model\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Fallback to manual formatting if template application fails\n",
    "        logger.warning(f\"tokenizer.apply_chat_template failed: {e}. Using manual format.\")\n",
    "        formatted_prompt = f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_model(model_name_or_path, base_model_name, device, is_adapter=False, hf_token=None):\n",
    "    \"\"\"Loads either the base model or a PEFT adapter model.\"\"\"\n",
    "    logger.info(f\"Loading model: {model_name_or_path}\")\n",
    "\n",
    "    # Determine dtype\n",
    "    model_dtype = torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "    if device.type == \"cuda\" and torch.cuda.is_bf16_supported():\n",
    "        model_dtype = torch.bfloat16\n",
    "\n",
    "    model_load_params = {\n",
    "        \"torch_dtype\": model_dtype,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"attn_implementation\": \"eager\", # Keep consistent with training\n",
    "    }\n",
    "    if hf_token:\n",
    "        model_load_params[\"token\"] = hf_token\n",
    "\n",
    "    # Load base model first\n",
    "    # Use try-except for robustness\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            **model_load_params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal: Failed to load base model {base_model_name}. Error: {e}\")\n",
    "        raise # Re-raise as this is critical\n",
    "\n",
    "    if is_adapter:\n",
    "        # Load PEFT model by applying adapter to the base model\n",
    "        try:\n",
    "            # Ensure adapter path exists before attempting to load\n",
    "            if not os.path.isdir(model_name_or_path):\n",
    "                 raise FileNotFoundError(f\"Adapter directory not found: {model_name_or_path}\")\n",
    "            model = PeftModel.from_pretrained(base_model, model_name_or_path)\n",
    "            logger.info(f\"Successfully loaded adapter from {model_name_or_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load adapter from {model_name_or_path}: {e}\")\n",
    "            logger.warning(\"Returning base model instead due to adapter load failure.\")\n",
    "            model = base_model # Fallback to base model if adapter fails\n",
    "    else:\n",
    "        # Using the base model directly\n",
    "        model = base_model\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    logger.info(\"Model loaded and set to evaluation mode.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, tokenizer, dataset, config, device):\n",
    "    \"\"\"Generate predictions for the entire dataset.\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # --- Corrected GenerationConfig setup ---\n",
    "    gen_config_params = {\n",
    "        \"max_new_tokens\": config.max_new_tokens,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    if config.do_sample:\n",
    "        gen_config_params[\"temperature\"] = config.temperature\n",
    "        gen_config_params[\"do_sample\"] = True\n",
    "    else:\n",
    "        gen_config_params[\"do_sample\"] = False\n",
    "    generation_config = GenerationConfig(**gen_config_params)\n",
    "    logger.info(f\"Using GenerationConfig: {generation_config}\")\n",
    "    # --- End of Correction ---\n",
    "\n",
    "\n",
    "    # Calculate max length for input tokens to leave space for generation\n",
    "    max_input_length = config.max_seq_length - config.max_new_tokens\n",
    "    if max_input_length <= 0:\n",
    "        raise ValueError(f\"max_seq_length ({config.max_seq_length}) must be greater than max_new_tokens ({config.max_new_tokens})\")\n",
    "\n",
    "    logger.info(f\"Generating predictions for {len(dataset)} examples...\")\n",
    "    logger.info(f\"Max input length for tokenizer: {max_input_length}\")\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Generating\"):\n",
    "        # 1. Prepare input\n",
    "        formatted_prompt = format_input_for_generation(example, tokenizer)\n",
    "        # Tokenize with truncation based on calculated max_input_length\n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_length # Use calculated max input length\n",
    "        ).to(device)\n",
    "\n",
    "        # 2. Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "        # 3. Decode and Extract SQL\n",
    "        generated_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sql_match = re.search(r\"SQL:\\s*(.*?)(?:\\nExplanation:|$)\", generated_text, re.DOTALL | re.IGNORECASE)\n",
    "        if sql_match:\n",
    "            extracted_sql = sql_match.group(1).strip()\n",
    "        else:\n",
    "            potential_sql = generated_text.strip()\n",
    "            if \"\\nExplanation:\" in potential_sql:\n",
    "                 extracted_sql = potential_sql.split(\"\\nExplanation:\", 1)[0].strip()\n",
    "            elif \"\\n\\n\" in potential_sql:\n",
    "                 extracted_sql = potential_sql.split(\"\\n\\n\", 1)[0].strip()\n",
    "            else:\n",
    "                 extracted_sql = potential_sql.split('\\n')[0].strip()\n",
    "            logger.debug(f\"Could not find 'SQL:' prefix. Extracted: {extracted_sql[:100]}...\")\n",
    "\n",
    "        predictions.append(extracted_sql)\n",
    "        references.append(example['sql']) # Store reference SQL\n",
    "\n",
    "    logger.info(\"Generation complete.\")\n",
    "    return predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sql(query):\n",
    "    \"\"\"Normalize SQL query for comparison.\"\"\"\n",
    "    try:\n",
    "        normalized = sqlparse.format(\n",
    "            str(query),\n",
    "            keyword_case='lower',\n",
    "            identifier_case='lower',\n",
    "            reindent=True,\n",
    "            strip_comments=True\n",
    "        )\n",
    "        normalized = normalized.strip().rstrip(';')\n",
    "        normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "        return normalized\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"SQL normalization failed for query: {str(query)[:100]}... Error: {e}\")\n",
    "        return str(query).strip().rstrip(';').lower()\n",
    "\n",
    "def compute_metrics(predictions, references, metrics_to_compute):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    results = {}\n",
    "    logger.info(f\"Computing metrics: {metrics_to_compute}\")\n",
    "\n",
    "    logger.info(\"Normalizing SQL queries for Exact Match...\")\n",
    "    norm_predictions = [normalize_sql(p) for p in tqdm(predictions, desc=\"Normalizing Preds\")]\n",
    "    norm_references = [normalize_sql(r) for r in tqdm(references, desc=\"Normalizing Refs\")]\n",
    "    logger.info(\"Normalization complete.\")\n",
    "\n",
    "    if \"exact_match\" in metrics_to_compute:\n",
    "        exact_match_count = sum(1 for pred, ref in zip(norm_predictions, norm_references) if pred == ref)\n",
    "        results[\"exact_match\"] = exact_match_count / len(references) if references else 0\n",
    "        logger.info(f\"Exact Match (Normalized): {results['exact_match']:.4f}\")\n",
    "\n",
    "    if \"bleu\" in metrics_to_compute:\n",
    "        try:\n",
    "            logger.info(\"Loading BLEU metric...\")\n",
    "            bleu_metric = evaluate.load(\"bleu\")\n",
    "            bleu_references = [[ref] for ref in references]\n",
    "            logger.info(\"Calculating BLEU score...\")\n",
    "            bleu_score = bleu_metric.compute(predictions=predictions, references=bleu_references)\n",
    "            results[\"bleu\"] = bleu_score['bleu']\n",
    "            logger.info(f\"BLEU Score: {results['bleu']:.4f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to compute BLEU: {e}\")\n",
    "            results[\"bleu\"] = 0.0\n",
    "\n",
    "    if \"rouge\" in metrics_to_compute:\n",
    "        try:\n",
    "            logger.info(\"Loading ROUGE metric...\")\n",
    "            rouge_metric = evaluate.load(\"rouge\")\n",
    "            logger.info(\"Calculating ROUGE scores...\")\n",
    "            rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "            results.update(rouge_score)\n",
    "            logger.info(f\"ROUGE Scores: R1={results.get('rouge1', 0.0):.4f}, R2={results.get('rouge2', 0.0):.4f}, RL={results.get('rougeL', 0.0):.4f}\")\n",
    "        except ModuleNotFoundError:\n",
    "             logger.error(\"ROUGE calculation failed: Missing dependencies. Try `pip install rouge_score absl-py nltk`\")\n",
    "             results.update({\"rouge1\": \"Error\", \"rouge2\": \"Error\", \"rougeL\": \"Error\", \"rougeLsum\": \"Error\"})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to compute ROUGE: {e}\")\n",
    "            results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation():\n",
    "    \"\"\"Run the full evaluation pipeline.\"\"\"\n",
    "    config = EvalConfig()\n",
    "    logger.info(f\"Evaluation config: {vars(config)}\")\n",
    "\n",
    "    # --- Create output directory ---\n",
    "    eval_output_dir = config.evaluation_output_dir\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    logger.info(f\"Evaluation outputs will be saved to: {eval_output_dir}\")\n",
    "    # --- End directory creation ---\n",
    "\n",
    "    if config.hf_token is None:\n",
    "        load_dotenv()\n",
    "        config.hf_token = os.getenv('HF_TOKEN')\n",
    "        if config.hf_token: logger.info(\"Loaded HF Token from .env\")\n",
    "\n",
    "    device = setup_eval_environment(config)\n",
    "    test_dataset = load_test_data(config)\n",
    "\n",
    "    # Load tokenizer\n",
    "    logger.info(f\"Loading tokenizer: {config.base_model_name}\")\n",
    "    try:\n",
    "        tokenizer_load_params = {}\n",
    "        if config.hf_token: tokenizer_load_params['token'] = config.hf_token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name, **tokenizer_load_params)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            logger.info(\"Set pad_token to eos_token.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal: Failed to load tokenizer. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    all_results = {}\n",
    "    models_to_evaluate = {\"baseline\": config.base_model_name}\n",
    "    models_to_evaluate.update(config.adapter_paths)\n",
    "\n",
    "    for model_key, model_path in models_to_evaluate.items():\n",
    "        logger.info(f\"\\n--- Evaluating Model: {model_key} ---\")\n",
    "        logger.info(f\"Path/Name: {model_path}\")\n",
    "\n",
    "        model = None\n",
    "        gc.collect()\n",
    "        if device.type == \"mps\": torch.mps.empty_cache()\n",
    "        elif device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "        is_adapter = (model_key != \"baseline\")\n",
    "        try:\n",
    "            model = load_evaluation_model(\n",
    "                model_path,\n",
    "                config.base_model_name,\n",
    "                device,\n",
    "                is_adapter=is_adapter,\n",
    "                hf_token=config.hf_token\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {model_key}. Skipping. Error: {e}\", exc_info=True)\n",
    "            all_results[model_key] = {\"error\": f\"Model loading failed: {e}\"}\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            predictions, references = generate_predictions(model, tokenizer, test_dataset, config, device)\n",
    "\n",
    "            # --- Save predictions to evaluation directory ---\n",
    "            try:\n",
    "                pred_df = pd.DataFrame({'reference': references, 'prediction': predictions})\n",
    "                # Use os.path.join to save inside the evaluation directory\n",
    "                pred_filename = os.path.join(eval_output_dir, f\"predictions_{model_key}.csv\")\n",
    "                pred_df.to_csv(pred_filename, index=False)\n",
    "                logger.info(f\"Saved predictions for {model_key} to {pred_filename}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save predictions for {model_key}: {e}\")\n",
    "            # --- End save predictions ---\n",
    "\n",
    "            metrics = compute_metrics(predictions, references, config.metrics_to_compute)\n",
    "            all_results[model_key] = metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during generation or metric calculation for {model_key}: {e}\", exc_info=True)\n",
    "            all_results[model_key] = {\"error\": f\"Generation/Metrics failed: {e}\"}\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == \"mps\": torch.mps.empty_cache()\n",
    "        elif device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "        logger.info(f\"--- Finished Evaluating Model: {model_key} ---\")\n",
    "\n",
    "\n",
    "    # --- Display Results ---\n",
    "    logger.info(\"\\n--- Evaluation Summary ---\")\n",
    "    results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
    "\n",
    "    float_cols = results_df.select_dtypes(include=['number']).columns\n",
    "    for col in float_cols:\n",
    "         if col in results_df.columns:\n",
    "              results_df[col] = results_df[col].apply(lambda x: f\"{x:.4f}\" if isinstance(x, (int, float, np.number)) else x)\n",
    "\n",
    "    print(results_df.to_markdown())\n",
    "\n",
    "    # --- Save results to evaluation directory ---\n",
    "    # Use os.path.join to save inside the evaluation directory\n",
    "    results_file = os.path.join(eval_output_dir, \"evaluation_results.json\")\n",
    "    try:\n",
    "        serializable_results = {}\n",
    "        for model, metrics in all_results.items():\n",
    "            serializable_results[model] = {k: (float(v) if isinstance(v, (np.number, np.float32, np.float64)) else v) for k, v in metrics.items()}\n",
    "\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(serializable_results, f, indent=4)\n",
    "        logger.info(f\"Evaluation results saved to {results_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results to JSON: {e}\")\n",
    "    # --- End save results ---\n",
    "\n",
    "    logger.info(\"Evaluation script finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:09:11,702 - __main__ - INFO - Evaluation config: {}\n",
      "2025-04-22 00:09:11,702 - __main__ - INFO - Evaluation outputs will be saved to: evaluation\n",
      "2025-04-22 00:09:11,703 - __main__ - INFO - Loaded HF Token from .env\n",
      "2025-04-22 00:09:11,705 - __main__ - INFO - PYTORCH_ENABLE_MPS_FALLBACK enabled.\n",
      "2025-04-22 00:09:11,723 - __main__ - INFO - Using MPS device.\n",
      "2025-04-22 00:09:11,723 - __main__ - INFO - Loading dataset: gretelai/synthetic_text_to_sql\n",
      "2025-04-22 00:09:13,467 - __main__ - INFO - Full test set size: 5851\n",
      "2025-04-22 00:09:13,468 - __main__ - INFO - Selecting subset of 15 examples for test dataset.\n",
      "2025-04-22 00:09:13,471 - __main__ - INFO - Using 15 examples for evaluation.\n",
      "2025-04-22 00:09:13,471 - __main__ - INFO - Loading tokenizer: google/gemma-3-1b-it\n",
      "2025-04-22 00:09:14,303 - __main__ - INFO - \n",
      "--- Evaluating Model: baseline ---\n",
      "2025-04-22 00:09:14,303 - __main__ - INFO - Path/Name: google/gemma-3-1b-it\n",
      "2025-04-22 00:09:14,386 - __main__ - INFO - Loading model: google/gemma-3-1b-it\n",
      "2025-04-22 00:09:15,866 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "2025-04-22 00:09:15,866 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:09:15,867 - __main__ - INFO - Generating predictions for 15 examples...\n",
      "2025-04-22 00:09:15,867 - __main__ - INFO - Max input length for tokenizer: 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c298b3c1dd847aea176cf1ce273c52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'cache_implementation': 'hybrid', 'top_k': 64, 'top_p': 0.95, 'bos_token_id': 2}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:14:14,933 - __main__ - INFO - Generation complete.\n",
      "2025-04-22 00:14:14,937 - __main__ - INFO - Saved predictions for baseline to evaluation/predictions_baseline.csv\n",
      "2025-04-22 00:14:14,937 - __main__ - INFO - Computing metrics: ['bleu', 'rouge', 'exact_match']\n",
      "2025-04-22 00:14:14,937 - __main__ - INFO - Normalizing SQL queries for Exact Match...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16db41b6dbae473586d9bc1659760082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Preds:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f133ab4fc844adea9ecfc18e7d496fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Refs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:14:14,970 - __main__ - INFO - Normalization complete.\n",
      "2025-04-22 00:14:14,970 - __main__ - INFO - Exact Match (Normalized): 0.0000\n",
      "2025-04-22 00:14:14,970 - __main__ - INFO - Loading BLEU metric...\n",
      "2025-04-22 00:14:16,052 - __main__ - INFO - Calculating BLEU score...\n",
      "2025-04-22 00:14:16,065 - __main__ - INFO - BLEU Score: 0.2026\n",
      "2025-04-22 00:14:16,065 - __main__ - INFO - Loading ROUGE metric...\n",
      "2025-04-22 00:14:16,529 - __main__ - INFO - Calculating ROUGE scores...\n",
      "2025-04-22 00:14:16,532 - absl - INFO - Using default tokenizer.\n",
      "2025-04-22 00:14:16,569 - __main__ - INFO - ROUGE Scores: R1=0.4282, R2=0.2836, RL=0.3917\n",
      "2025-04-22 00:14:16,705 - __main__ - INFO - --- Finished Evaluating Model: baseline ---\n",
      "2025-04-22 00:14:16,706 - __main__ - INFO - \n",
      "--- Evaluating Model: LoRA_r4_lr2e-5 ---\n",
      "2025-04-22 00:14:16,706 - __main__ - INFO - Path/Name: models/gemma3_finetuned_20250421_201739_r4_lr2e-5/final_adapter\n",
      "2025-04-22 00:14:16,785 - __main__ - INFO - Loading model: models/gemma3_finetuned_20250421_201739_r4_lr2e-5/final_adapter\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "2025-04-22 00:14:17,578 - __main__ - INFO - Successfully loaded adapter from models/gemma3_finetuned_20250421_201739_r4_lr2e-5/final_adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udaykiran/Documents/NEU/Prompt Engineering/Finetuning/.venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:14:17,884 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "2025-04-22 00:14:17,885 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:14:17,885 - __main__ - INFO - Generating predictions for 15 examples...\n",
      "2025-04-22 00:14:17,885 - __main__ - INFO - Max input length for tokenizer: 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6da1f441394057b300d213407fd0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:19:35,945 - __main__ - INFO - Generation complete.\n",
      "2025-04-22 00:19:35,946 - __main__ - INFO - Saved predictions for LoRA_r4_lr2e-5 to evaluation/predictions_LoRA_r4_lr2e-5.csv\n",
      "2025-04-22 00:19:35,947 - __main__ - INFO - Computing metrics: ['bleu', 'rouge', 'exact_match']\n",
      "2025-04-22 00:19:35,947 - __main__ - INFO - Normalizing SQL queries for Exact Match...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2f5fc293b7424c84fa18aa28ac078b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Preds:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cb1cae4a0748cf8058b0183876c586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Refs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:19:35,980 - __main__ - INFO - Normalization complete.\n",
      "2025-04-22 00:19:35,980 - __main__ - INFO - Exact Match (Normalized): 0.0000\n",
      "2025-04-22 00:19:35,980 - __main__ - INFO - Loading BLEU metric...\n",
      "2025-04-22 00:19:36,775 - __main__ - INFO - Calculating BLEU score...\n",
      "2025-04-22 00:19:36,790 - __main__ - INFO - BLEU Score: 0.1704\n",
      "2025-04-22 00:19:36,791 - __main__ - INFO - Loading ROUGE metric...\n",
      "2025-04-22 00:19:37,149 - __main__ - INFO - Calculating ROUGE scores...\n",
      "2025-04-22 00:19:37,156 - absl - INFO - Using default tokenizer.\n",
      "2025-04-22 00:19:37,213 - __main__ - INFO - ROUGE Scores: R1=0.3590, R2=0.2268, RL=0.3237\n",
      "2025-04-22 00:19:37,341 - __main__ - INFO - --- Finished Evaluating Model: LoRA_r4_lr2e-5 ---\n",
      "2025-04-22 00:19:37,342 - __main__ - INFO - \n",
      "--- Evaluating Model: LoRA_r8_lr2e-5 ---\n",
      "2025-04-22 00:19:37,342 - __main__ - INFO - Path/Name: models/gemma3_finetuned_20250421_212338_r8_lr2e-5/final_adapter\n",
      "2025-04-22 00:19:37,422 - __main__ - INFO - Loading model: models/gemma3_finetuned_20250421_212338_r8_lr2e-5/final_adapter\n",
      "2025-04-22 00:19:38,091 - __main__ - INFO - Successfully loaded adapter from models/gemma3_finetuned_20250421_212338_r8_lr2e-5/final_adapter\n",
      "2025-04-22 00:19:38,400 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "2025-04-22 00:19:38,400 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:19:38,400 - __main__ - INFO - Generating predictions for 15 examples...\n",
      "2025-04-22 00:19:38,401 - __main__ - INFO - Max input length for tokenizer: 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6b0a217ab24d789d62f56d4fd84814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:25:12,205 - __main__ - INFO - Generation complete.\n",
      "2025-04-22 00:25:12,207 - __main__ - INFO - Saved predictions for LoRA_r8_lr2e-5 to evaluation/predictions_LoRA_r8_lr2e-5.csv\n",
      "2025-04-22 00:25:12,207 - __main__ - INFO - Computing metrics: ['bleu', 'rouge', 'exact_match']\n",
      "2025-04-22 00:25:12,207 - __main__ - INFO - Normalizing SQL queries for Exact Match...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0485b9d84630462f9c44f11de97f15ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Preds:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a089f90bd3dc417b9a08e981840ccfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Refs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:25:12,261 - __main__ - INFO - Normalization complete.\n",
      "2025-04-22 00:25:12,262 - __main__ - INFO - Exact Match (Normalized): 0.0000\n",
      "2025-04-22 00:25:12,262 - __main__ - INFO - Loading BLEU metric...\n",
      "2025-04-22 00:25:19,403 - __main__ - INFO - Calculating BLEU score...\n",
      "2025-04-22 00:25:19,419 - __main__ - INFO - BLEU Score: 0.1700\n",
      "2025-04-22 00:25:19,420 - __main__ - INFO - Loading ROUGE metric...\n",
      "2025-04-22 00:25:19,802 - __main__ - INFO - Calculating ROUGE scores...\n",
      "2025-04-22 00:25:19,808 - absl - INFO - Using default tokenizer.\n",
      "2025-04-22 00:25:19,863 - __main__ - INFO - ROUGE Scores: R1=0.5166, R2=0.3289, RL=0.4806\n",
      "2025-04-22 00:25:20,092 - __main__ - INFO - --- Finished Evaluating Model: LoRA_r8_lr2e-5 ---\n",
      "2025-04-22 00:25:20,093 - __main__ - INFO - \n",
      "--- Evaluating Model: LoRA_r16_lr2e-5 ---\n",
      "2025-04-22 00:25:20,093 - __main__ - INFO - Path/Name: models/gemma3_finetuned_20250421_224200_r16_lr2e-5/final_adapter\n",
      "2025-04-22 00:25:20,211 - __main__ - INFO - Loading model: models/gemma3_finetuned_20250421_224200_r16_lr2e-5/final_adapter\n",
      "2025-04-22 00:25:20,897 - __main__ - INFO - Successfully loaded adapter from models/gemma3_finetuned_20250421_224200_r16_lr2e-5/final_adapter\n",
      "2025-04-22 00:25:21,156 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "2025-04-22 00:25:21,157 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:25:21,157 - __main__ - INFO - Generating predictions for 15 examples...\n",
      "2025-04-22 00:25:21,157 - __main__ - INFO - Max input length for tokenizer: 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be47bcbc40d04bd49b7bb235aaf5ac4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:31:18,818 - __main__ - INFO - Generation complete.\n",
      "2025-04-22 00:31:18,820 - __main__ - INFO - Saved predictions for LoRA_r16_lr2e-5 to evaluation/predictions_LoRA_r16_lr2e-5.csv\n",
      "2025-04-22 00:31:18,820 - __main__ - INFO - Computing metrics: ['bleu', 'rouge', 'exact_match']\n",
      "2025-04-22 00:31:18,821 - __main__ - INFO - Normalizing SQL queries for Exact Match...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536273a6ec954aa1af5883083121cd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Preds:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc8b9efaf2144baba008e4169645c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Refs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:31:18,856 - __main__ - INFO - Normalization complete.\n",
      "2025-04-22 00:31:18,857 - __main__ - INFO - Exact Match (Normalized): 0.0000\n",
      "2025-04-22 00:31:18,857 - __main__ - INFO - Loading BLEU metric...\n",
      "2025-04-22 00:31:19,908 - __main__ - INFO - Calculating BLEU score...\n",
      "2025-04-22 00:31:19,922 - __main__ - INFO - BLEU Score: 0.2186\n",
      "2025-04-22 00:31:19,923 - __main__ - INFO - Loading ROUGE metric...\n",
      "2025-04-22 00:31:20,390 - __main__ - INFO - Calculating ROUGE scores...\n",
      "2025-04-22 00:31:20,393 - absl - INFO - Using default tokenizer.\n",
      "2025-04-22 00:31:20,442 - __main__ - INFO - ROUGE Scores: R1=0.4673, R2=0.3283, RL=0.4454\n",
      "2025-04-22 00:31:20,586 - __main__ - INFO - --- Finished Evaluating Model: LoRA_r16_lr2e-5 ---\n",
      "2025-04-22 00:31:20,586 - __main__ - INFO - \n",
      "--- Evaluation Summary ---\n",
      "|                 |   exact_match |   bleu |   rouge1 |   rouge2 |   rougeL |   rougeLsum |\n",
      "|:----------------|--------------:|-------:|---------:|---------:|---------:|------------:|\n",
      "| baseline        |             0 | 0.2026 |   0.4282 |   0.2836 |   0.3917 |      0.3965 |\n",
      "| LoRA_r4_lr2e-5  |             0 | 0.1704 |   0.359  |   0.2268 |   0.3237 |      0.3372 |\n",
      "| LoRA_r8_lr2e-5  |             0 | 0.17   |   0.5166 |   0.3289 |   0.4806 |      0.4916 |\n",
      "| LoRA_r16_lr2e-5 |             0 | 0.2186 |   0.4673 |   0.3283 |   0.4454 |      0.4195 |\n",
      "2025-04-22 00:31:20,592 - __main__ - INFO - Evaluation results saved to evaluation/evaluation_results.json\n",
      "2025-04-22 00:31:20,593 - __main__ - INFO - Evaluation script finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if running in an interactive environment (like Jupyter)\n",
    "    if 'get_ipython' in globals() or 'google.colab' in sys.modules or os.environ.get(\"IPYKERNEL_CELL_NAME\"):\n",
    "        run_evaluation()\n",
    "    else:\n",
    "        logger.info(\"Script appears to be running in a non-interactive environment.\")\n",
    "        logger.info(\"Running evaluation...\")\n",
    "        run_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
