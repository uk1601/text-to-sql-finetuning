{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm for better display\n",
    "from typing import Dict, List, Optional, Any # Added Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig # If considering quantization\n",
    ")\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    \"\"\"Configuration settings for the inference script.\"\"\"\n",
    "    # --- Model Identification ---\n",
    "    base_model_name: str = \"google/gemma-3-1b-it\"\n",
    "    # Dictionary mapping user-friendly names to model paths\n",
    "    # Baseline uses the base model name, adapters use relative paths\n",
    "    model_options: Dict[str, str] = {\n",
    "        \"baseline\": base_model_name,\n",
    "        \"lora_r4\": \"models/gemma3_finetuned_20250421_163503_r4_lr2e-5/final_adapter\",\n",
    "        \"lora_r8\": \"models/gemma3_finetuned_20250421_164347_r8_lr2e-5/final_adapter\",\n",
    "        \"lora_r16\": \"models/gemma3_finetuned_20250421_163914_r16_lr2e-5/final_adapter\",\n",
    "    }\n",
    "\n",
    "    # --- Dataset ---\n",
    "    dataset_name: str = \"gretelai/synthetic_text_to_sql\"\n",
    "    # Used only if selecting example by index\n",
    "    seed: int = 42\n",
    "\n",
    "    # --- Generation Parameters ---\n",
    "    max_new_tokens: int = 256 # Max tokens for generated SQL + Explanation\n",
    "    temperature: float = 0.1 # Low temperature for more deterministic output\n",
    "    do_sample: bool = False # Use greedy decoding for consistency\n",
    "    max_seq_length: int = 512 # Max overall sequence length (for input truncation)\n",
    "\n",
    "    # --- Environment ---\n",
    "    use_mps_fallback: bool = True # Enable MPS fallback if needed\n",
    "    hf_token: Optional[str] = None # Loads from .env if None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_inference_environment(config: InferenceConfig):\n",
    "    \"\"\"Set up seeds, device, and MPS fallback.\"\"\"\n",
    "    random.seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    torch.manual_seed(config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "    if config.use_mps_fallback:\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        logger.info(\"PYTORCH_ENABLE_MPS_FALLBACK enabled.\")\n",
    "        os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(\"Using MPS device.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU device.\")\n",
    "\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inference_tokenizer(config: InferenceConfig):\n",
    "    \"\"\"Loads the tokenizer.\"\"\"\n",
    "    logger.info(f\"Loading tokenizer: {config.base_model_name}\")\n",
    "    try:\n",
    "        tokenizer_load_params = {}\n",
    "        if config.hf_token: tokenizer_load_params['token'] = config.hf_token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name, **tokenizer_load_params)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            logger.info(\"Set pad_token to eos_token.\")\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal: Failed to load tokenizer. Error: {e}\")\n",
    "        raise # Re-raise critical error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inference_model(selected_model_path: str, config: InferenceConfig, device):\n",
    "    \"\"\"Loads the selected model (base or PEFT adapter).\"\"\"\n",
    "    is_adapter = (selected_model_path != config.base_model_name)\n",
    "    model_id_to_load = config.base_model_name # Always load base first\n",
    "\n",
    "    logger.info(f\"Loading base model: {model_id_to_load}\")\n",
    "\n",
    "    # Determine dtype\n",
    "    model_dtype = torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "    if device.type == \"cuda\" and torch.cuda.is_bf16_supported():\n",
    "        model_dtype = torch.bfloat16\n",
    "\n",
    "    model_load_params = {\n",
    "        \"torch_dtype\": model_dtype,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"attn_implementation\": \"eager\",\n",
    "    }\n",
    "    if config.hf_token:\n",
    "        model_load_params[\"token\"] = config.hf_token\n",
    "\n",
    "    # Load base model\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id_to_load,\n",
    "            **model_load_params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal: Failed to load base model {model_id_to_load}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "    if is_adapter:\n",
    "        logger.info(f\"Loading adapter: {selected_model_path}\")\n",
    "        try:\n",
    "            if not os.path.isdir(selected_model_path):\n",
    "                 raise FileNotFoundError(f\"Adapter directory not found: {selected_model_path}\")\n",
    "            # Load PEFT model by applying adapter to the base model\n",
    "            model = PeftModel.from_pretrained(model, selected_model_path)\n",
    "            logger.info(f\"Successfully loaded adapter from {selected_model_path}\")\n",
    "            # Optional: Merge adapter if desired (faster inference, more memory)\n",
    "            # logger.info(\"Merging adapter...\")\n",
    "            # model = model.merge_and_unload()\n",
    "            # logger.info(\"Adapter merged.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load adapter from {selected_model_path}. Error: {e}\")\n",
    "            logger.warning(\"Proceeding with base model only.\")\n",
    "            # Model variable already holds the base model\n",
    "    else:\n",
    "        logger.info(\"Using base model directly.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    logger.info(\"Model loaded and set to evaluation mode.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_from_dataset(config: InferenceConfig, index: int):\n",
    "    \"\"\"Loads a specific example from the test dataset.\"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(config.dataset_name)\n",
    "        split_name = 'test'\n",
    "        if split_name not in dataset:\n",
    "            split_name = 'train'\n",
    "            logger.warning(f\"'test' split not found, using '{split_name}' split.\")\n",
    "\n",
    "        data_split = dataset[split_name]\n",
    "        if not 0 <= index < len(data_split):\n",
    "            logger.error(f\"Index {index} out of range for {split_name} split (size {len(data_split)}).\")\n",
    "            return None, None # Return None for example and reference\n",
    "\n",
    "        example = data_split[index]\n",
    "        reference_sql = example.get('sql', None)\n",
    "        logger.info(f\"Loaded example {index} from {split_name} split.\")\n",
    "        return example, reference_sql\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset example: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_inference_prompt(tokenizer, prompt: str, context: Optional[str] = None):\n",
    "    \"\"\"Formats the prompt using the chat template for generation.\"\"\"\n",
    "    user_message = f\"Generate the SQL query for the following request\"\n",
    "    if context:\n",
    "        user_message += f\" based on the provided context.\\n\\nRequest: {prompt}\\n\\nDatabase Context:\\n{context}\"\n",
    "    else:\n",
    "        user_message += f\".\\n\\nRequest: {prompt}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    try:\n",
    "        # Apply chat template, ensuring it adds the prompt for the model to respond\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True # Crucial for inference\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"tokenizer.apply_chat_template failed: {e}. Using manual format.\")\n",
    "        formatted_prompt = f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt_text: str, config: InferenceConfig, device):\n",
    "    \"\"\"Generates text using the specified model and configuration.\"\"\"\n",
    "    logger.info(\"Starting generation...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare generation config\n",
    "    gen_config_params = {\n",
    "        \"max_new_tokens\": config.max_new_tokens,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    if config.do_sample:\n",
    "        gen_config_params[\"temperature\"] = config.temperature\n",
    "        gen_config_params[\"do_sample\"] = True\n",
    "    else:\n",
    "        gen_config_params[\"do_sample\"] = False\n",
    "    generation_config = GenerationConfig(**gen_config_params)\n",
    "    logger.info(f\"Using GenerationConfig: {generation_config}\")\n",
    "\n",
    "\n",
    "    # Tokenize input\n",
    "    max_input_length = config.max_seq_length - config.max_new_tokens\n",
    "    if max_input_length <= 0:\n",
    "        logger.error(f\"max_seq_length ({config.max_seq_length}) too small for max_new_tokens ({config.max_new_tokens}).\")\n",
    "        return \"Error: Configuration invalid (max_seq_length too small).\", 0\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "        inference_time = time.time() - start_time\n",
    "        logger.info(f\"Generation finished in {inference_time:.2f} seconds.\")\n",
    "\n",
    "        # Decode generated part\n",
    "        generated_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        return generated_text, inference_time\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during model.generate: {e}\", exc_info=True)\n",
    "        return f\"Error during generation: {e}\", time.time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sql_from_output(generated_text: str):\n",
    "    \"\"\"Extracts the SQL query from the model's generated text.\"\"\"\n",
    "    logger.debug(f\"Attempting to extract SQL from: {generated_text[:200]}...\")\n",
    "    # Try to find \"SQL:\" prefix first\n",
    "    sql_match = re.search(r\"SQL:\\s*(.*?)(?:\\nExplanation:|$)\", generated_text, re.DOTALL | re.IGNORECASE)\n",
    "    if sql_match:\n",
    "        extracted_sql = sql_match.group(1).strip()\n",
    "        logger.debug(\"Found SQL using 'SQL:' prefix.\")\n",
    "    else:\n",
    "        # Fallback logic if \"SQL:\" prefix is missing\n",
    "        potential_sql = generated_text.strip()\n",
    "        if \"\\nExplanation:\" in potential_sql:\n",
    "             extracted_sql = potential_sql.split(\"\\nExplanation:\", 1)[0].strip()\n",
    "             logger.debug(\"Found SQL by splitting at 'Explanation:'.\")\n",
    "        elif \"\\n\\n\" in potential_sql: # Check for double newline as separator\n",
    "             extracted_sql = potential_sql.split(\"\\n\\n\", 1)[0].strip()\n",
    "             logger.debug(\"Found SQL by splitting at double newline.\")\n",
    "        else: # Otherwise take the first line\n",
    "             extracted_sql = potential_sql.split('\\n')[0].strip()\n",
    "             logger.debug(\"Using first line as SQL (fallback).\")\n",
    "    # Final cleanup (remove potential trailing markdown/code fences)\n",
    "    extracted_sql = re.sub(r\"```$\", \"\", extracted_sql.strip()).strip()\n",
    "    return extracted_sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_inference():\n",
    "    \"\"\"Handles user interaction for model/input selection and runs inference.\"\"\"\n",
    "    config = InferenceConfig()\n",
    "    logger.info(f\"Inference config: {vars(config)}\")\n",
    "\n",
    "    if config.hf_token is None:\n",
    "        load_dotenv()\n",
    "        config.hf_token = os.getenv('HF_TOKEN')\n",
    "        if config.hf_token: logger.info(\"Loaded HF Token from .env\")\n",
    "\n",
    "    device = setup_inference_environment(config)\n",
    "    tokenizer = load_inference_tokenizer(config)\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    print(\"\\nAvailable models:\")\n",
    "    model_choices = list(config.model_options.keys())\n",
    "    for i, name in enumerate(model_choices):\n",
    "        print(f\"{i+1}. {name}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_choice_idx = int(input(f\"Select model (1-{len(model_choices)}): \")) - 1\n",
    "            if 0 <= model_choice_idx < len(model_choices):\n",
    "                selected_model_key = model_choices[model_choice_idx]\n",
    "                selected_model_path = config.model_options[selected_model_key]\n",
    "                logger.info(f\"User selected model: {selected_model_key} ({selected_model_path})\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    # --- Load Selected Model ---\n",
    "    model = None\n",
    "    try:\n",
    "        model = load_inference_model(selected_model_path, config, device)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error loading selected model: {e}\", exc_info=True)\n",
    "        return # Exit if model loading fails\n",
    "\n",
    "    # --- Input Selection ---\n",
    "    print(\"\\nSelect input method:\")\n",
    "    print(\"1. Custom prompt (and optional context)\")\n",
    "    print(\"2. Example from test dataset by index\")\n",
    "\n",
    "    input_text = None\n",
    "    reference_sql = None\n",
    "    example_details = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            input_choice = input(\"Select input method (1-2): \").strip()\n",
    "            if input_choice == '1':\n",
    "                prompt = input(\"Enter the SQL prompt/request: \")\n",
    "                context = input(\"Enter the SQL context (CREATE TABLE... statements) [Optional, press Enter to skip]: \")\n",
    "                input_text = format_inference_prompt(tokenizer, prompt.strip(), context.strip() if context else None)\n",
    "                example_details['prompt'] = prompt\n",
    "                example_details['context'] = context if context else \"None provided\"\n",
    "                break\n",
    "            elif input_choice == '2':\n",
    "                try:\n",
    "                    idx = int(input(f\"Enter test dataset index (0 to ~5850): \"))\n",
    "                    example, reference_sql = get_example_from_dataset(config, idx)\n",
    "                    if example:\n",
    "                        input_text = format_inference_prompt(tokenizer, example['sql_prompt'], example['sql_context'])\n",
    "                        example_details['prompt'] = example['sql_prompt']\n",
    "                        example_details['context'] = example['sql_context']\n",
    "                        example_details['index'] = idx\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Failed to load example. Please try again.\")\n",
    "                        # Loop continues\n",
    "                except ValueError:\n",
    "                    print(\"Invalid index. Please enter a number.\")\n",
    "            else:\n",
    "                print(\"Invalid choice.\")\n",
    "        except EOFError: # Handle cases where input stream ends unexpectedly\n",
    "             logger.error(\"Input stream closed unexpectedly.\")\n",
    "             return\n",
    "\n",
    "\n",
    "    # --- Run Inference ---\n",
    "    if input_text and model:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Running inference with model: {selected_model_key}\")\n",
    "        print(f\"Input Prompt:\\n{example_details.get('prompt', 'N/A')}\")\n",
    "        print(f\"Input Context:\\n{example_details.get('context', 'N/A')}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        generated_text, inference_time = generate_text(model, tokenizer, input_text, config, device)\n",
    "\n",
    "        print(f\"\\n--- Raw Model Output (took {inference_time:.2f}s) ---\")\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        extracted_sql = extract_sql_from_output(generated_text)\n",
    "        print(f\"\\n--- Extracted SQL ---\")\n",
    "        print(extracted_sql)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        if reference_sql:\n",
    "            print(f\"\\n--- Reference SQL (Example {example_details.get('index', 'N/A')}) ---\")\n",
    "            print(reference_sql)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "    elif not model:\n",
    "         logger.error(\"Model was not loaded successfully. Cannot run inference.\")\n",
    "    else:\n",
    "         logger.error(\"Input text was not prepared successfully. Cannot run inference.\")\n",
    "\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    logger.info(\"Cleaning up...\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if device.type == \"mps\": torch.mps.empty_cache()\n",
    "    elif device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "    logger.info(\"Inference complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:38:17,937 - __main__ - INFO - Inference config: {}\n",
      "2025-04-22 00:38:17,938 - __main__ - INFO - Loaded HF Token from .env\n",
      "2025-04-22 00:38:17,941 - __main__ - INFO - PYTORCH_ENABLE_MPS_FALLBACK enabled.\n",
      "2025-04-22 00:38:17,955 - __main__ - INFO - Using MPS device.\n",
      "2025-04-22 00:38:17,956 - __main__ - INFO - Loading tokenizer: google/gemma-3-1b-it\n",
      "\n",
      "Available models:\n",
      "1. baseline\n",
      "2. lora_r4\n",
      "3. lora_r8\n",
      "4. lora_r16\n",
      "2025-04-22 00:38:21,184 - __main__ - INFO - User selected model: baseline (google/gemma-3-1b-it)\n",
      "2025-04-22 00:38:21,184 - __main__ - INFO - Loading base model: google/gemma-3-1b-it\n",
      "2025-04-22 00:38:21,791 - __main__ - INFO - Using base model directly.\n",
      "2025-04-22 00:38:22,038 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "\n",
      "Select input method:\n",
      "1. Custom prompt (and optional context)\n",
      "2. Example from test dataset by index\n",
      "2025-04-22 00:38:28,990 - __main__ - INFO - Loaded example 1200 from test split.\n",
      "\n",
      "================================================================================\n",
      "Running inference with model: baseline\n",
      "Input Prompt:\n",
      "What is the maximum energy efficiency rating of hydroelectric dams in Canada?\n",
      "Input Context:\n",
      "CREATE TABLE hydro_dams (id INT, name TEXT, country TEXT, energy_efficiency_rating FLOAT); INSERT INTO hydro_dams (id, name, country, energy_efficiency_rating) VALUES (1, 'Robert-Bourassa', 'Canada', 0.94), (2, 'Churchill Falls', 'Canada', 0.92);\n",
      "================================================================================\n",
      "2025-04-22 00:38:28,997 - __main__ - INFO - Starting generation...\n",
      "2025-04-22 00:38:28,998 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'cache_implementation': 'hybrid', 'top_k': 64, 'top_p': 0.95, 'bos_token_id': 2}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:38:51,988 - __main__ - INFO - Generation finished in 22.99 seconds.\n",
      "\n",
      "--- Raw Model Output (took 22.99s) ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "```\n",
      "\n",
      " প্রবৃত্ত to generate the SQL query.अंतिम\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Extracted SQL ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Reference SQL (Example 1200) ---\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams WHERE country = 'Canada';\n",
      "--------------------------------------------------------------------------------\n",
      "2025-04-22 00:38:51,989 - __main__ - INFO - Cleaning up...\n",
      "2025-04-22 00:38:52,119 - __main__ - INFO - Inference complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     # Check if running in an interactive environment (like Jupyter)\n",
    "    if 'get_ipython' in globals() or 'google.colab' in sys.modules or os.environ.get(\"IPYKERNEL_CELL_NAME\"):\n",
    "        run_interactive_inference()\n",
    "    else:\n",
    "        logger.info(\"Script appears to be running in a non-interactive environment.\")\n",
    "        logger.info(\"Running interactive inference...\")\n",
    "        run_interactive_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:38:52,151 - __main__ - INFO - Inference config: {}\n",
      "2025-04-22 00:38:52,153 - __main__ - INFO - Loaded HF Token from .env\n",
      "2025-04-22 00:38:52,154 - __main__ - INFO - PYTORCH_ENABLE_MPS_FALLBACK enabled.\n",
      "2025-04-22 00:38:52,155 - __main__ - INFO - Using MPS device.\n",
      "2025-04-22 00:38:52,159 - __main__ - INFO - Loading tokenizer: google/gemma-3-1b-it\n",
      "\n",
      "Available models:\n",
      "1. baseline\n",
      "2. lora_r4\n",
      "3. lora_r8\n",
      "4. lora_r16\n",
      "2025-04-22 00:39:22,338 - __main__ - INFO - User selected model: lora_r4 (models/gemma3_finetuned_20250421_163503_r4_lr2e-5/final_adapter)\n",
      "2025-04-22 00:39:22,339 - __main__ - INFO - Loading base model: google/gemma-3-1b-it\n",
      "2025-04-22 00:39:22,763 - __main__ - INFO - Loading adapter: models/gemma3_finetuned_20250421_163503_r4_lr2e-5/final_adapter\n",
      "2025-04-22 00:39:22,764 - __main__ - ERROR - Failed to load adapter from models/gemma3_finetuned_20250421_163503_r4_lr2e-5/final_adapter. Error: Adapter directory not found: models/gemma3_finetuned_20250421_163503_r4_lr2e-5/final_adapter\n",
      "2025-04-22 00:39:22,764 - __main__ - WARNING - Proceeding with base model only.\n",
      "2025-04-22 00:39:23,019 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "\n",
      "Select input method:\n",
      "1. Custom prompt (and optional context)\n",
      "2. Example from test dataset by index\n",
      "2025-04-22 00:39:28,404 - __main__ - INFO - Loaded example 1200 from test split.\n",
      "\n",
      "================================================================================\n",
      "Running inference with model: lora_r4\n",
      "Input Prompt:\n",
      "What is the maximum energy efficiency rating of hydroelectric dams in Canada?\n",
      "Input Context:\n",
      "CREATE TABLE hydro_dams (id INT, name TEXT, country TEXT, energy_efficiency_rating FLOAT); INSERT INTO hydro_dams (id, name, country, energy_efficiency_rating) VALUES (1, 'Robert-Bourassa', 'Canada', 0.94), (2, 'Churchill Falls', 'Canada', 0.92);\n",
      "================================================================================\n",
      "2025-04-22 00:39:28,406 - __main__ - INFO - Starting generation...\n",
      "2025-04-22 00:39:28,406 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:39:47,209 - __main__ - INFO - Generation finished in 18.80 seconds.\n",
      "\n",
      "--- Raw Model Output (took 18.80s) ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "```\n",
      "\n",
      " প্রবৃত্ত to generate the SQL query.अंतिम\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Extracted SQL ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Reference SQL (Example 1200) ---\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams WHERE country = 'Canada';\n",
      "--------------------------------------------------------------------------------\n",
      "2025-04-22 00:39:47,210 - __main__ - INFO - Cleaning up...\n",
      "2025-04-22 00:39:47,344 - __main__ - INFO - Inference complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     # Check if running in an interactive environment (like Jupyter)\n",
    "    if 'get_ipython' in globals() or 'google.colab' in sys.modules or os.environ.get(\"IPYKERNEL_CELL_NAME\"):\n",
    "        run_interactive_inference()\n",
    "    else:\n",
    "        logger.info(\"Script appears to be running in a non-interactive environment.\")\n",
    "        logger.info(\"Running interactive inference...\")\n",
    "        run_interactive_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:39:47,375 - __main__ - INFO - Inference config: {}\n",
      "2025-04-22 00:39:47,376 - __main__ - INFO - Loaded HF Token from .env\n",
      "2025-04-22 00:39:47,377 - __main__ - INFO - PYTORCH_ENABLE_MPS_FALLBACK enabled.\n",
      "2025-04-22 00:39:47,378 - __main__ - INFO - Using MPS device.\n",
      "2025-04-22 00:39:47,378 - __main__ - INFO - Loading tokenizer: google/gemma-3-1b-it\n",
      "\n",
      "Available models:\n",
      "1. baseline\n",
      "2. lora_r4\n",
      "3. lora_r8\n",
      "4. lora_r16\n",
      "2025-04-22 00:39:56,434 - __main__ - INFO - User selected model: lora_r8 (models/gemma3_finetuned_20250421_164347_r8_lr2e-5/final_adapter)\n",
      "2025-04-22 00:39:56,435 - __main__ - INFO - Loading base model: google/gemma-3-1b-it\n",
      "2025-04-22 00:39:56,877 - __main__ - INFO - Loading adapter: models/gemma3_finetuned_20250421_164347_r8_lr2e-5/final_adapter\n",
      "2025-04-22 00:39:56,878 - __main__ - ERROR - Failed to load adapter from models/gemma3_finetuned_20250421_164347_r8_lr2e-5/final_adapter. Error: Adapter directory not found: models/gemma3_finetuned_20250421_164347_r8_lr2e-5/final_adapter\n",
      "2025-04-22 00:39:56,878 - __main__ - WARNING - Proceeding with base model only.\n",
      "2025-04-22 00:39:57,137 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "\n",
      "Select input method:\n",
      "1. Custom prompt (and optional context)\n",
      "2. Example from test dataset by index\n",
      "2025-04-22 00:40:08,450 - __main__ - INFO - Loaded example 1200 from test split.\n",
      "\n",
      "================================================================================\n",
      "Running inference with model: lora_r8\n",
      "Input Prompt:\n",
      "What is the maximum energy efficiency rating of hydroelectric dams in Canada?\n",
      "Input Context:\n",
      "CREATE TABLE hydro_dams (id INT, name TEXT, country TEXT, energy_efficiency_rating FLOAT); INSERT INTO hydro_dams (id, name, country, energy_efficiency_rating) VALUES (1, 'Robert-Bourassa', 'Canada', 0.94), (2, 'Churchill Falls', 'Canada', 0.92);\n",
      "================================================================================\n",
      "2025-04-22 00:40:08,451 - __main__ - INFO - Starting generation...\n",
      "2025-04-22 00:40:08,452 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:40:27,626 - __main__ - INFO - Generation finished in 19.17 seconds.\n",
      "\n",
      "--- Raw Model Output (took 19.17s) ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "```\n",
      "\n",
      " প্রবৃত্ত to generate the SQL query.अंतिम\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Extracted SQL ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Reference SQL (Example 1200) ---\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams WHERE country = 'Canada';\n",
      "--------------------------------------------------------------------------------\n",
      "2025-04-22 00:40:27,627 - __main__ - INFO - Cleaning up...\n",
      "2025-04-22 00:40:27,757 - __main__ - INFO - Inference complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     # Check if running in an interactive environment (like Jupyter)\n",
    "    if 'get_ipython' in globals() or 'google.colab' in sys.modules or os.environ.get(\"IPYKERNEL_CELL_NAME\"):\n",
    "        run_interactive_inference()\n",
    "    else:\n",
    "        logger.info(\"Script appears to be running in a non-interactive environment.\")\n",
    "        logger.info(\"Running interactive inference...\")\n",
    "        run_interactive_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 00:40:27,790 - __main__ - INFO - Inference config: {}\n",
      "2025-04-22 00:40:27,792 - __main__ - INFO - Loaded HF Token from .env\n",
      "2025-04-22 00:40:27,794 - __main__ - INFO - PYTORCH_ENABLE_MPS_FALLBACK enabled.\n",
      "2025-04-22 00:40:27,794 - __main__ - INFO - Using MPS device.\n",
      "2025-04-22 00:40:27,794 - __main__ - INFO - Loading tokenizer: google/gemma-3-1b-it\n",
      "\n",
      "Available models:\n",
      "1. baseline\n",
      "2. lora_r4\n",
      "3. lora_r8\n",
      "4. lora_r16\n",
      "2025-04-22 00:40:32,419 - __main__ - INFO - User selected model: lora_r16 (models/gemma3_finetuned_20250421_163914_r16_lr2e-5/final_adapter)\n",
      "2025-04-22 00:40:32,419 - __main__ - INFO - Loading base model: google/gemma-3-1b-it\n",
      "2025-04-22 00:40:32,852 - __main__ - INFO - Loading adapter: models/gemma3_finetuned_20250421_163914_r16_lr2e-5/final_adapter\n",
      "2025-04-22 00:40:32,852 - __main__ - ERROR - Failed to load adapter from models/gemma3_finetuned_20250421_163914_r16_lr2e-5/final_adapter. Error: Adapter directory not found: models/gemma3_finetuned_20250421_163914_r16_lr2e-5/final_adapter\n",
      "2025-04-22 00:40:32,852 - __main__ - WARNING - Proceeding with base model only.\n",
      "2025-04-22 00:40:33,102 - __main__ - INFO - Model loaded and set to evaluation mode.\n",
      "\n",
      "Select input method:\n",
      "1. Custom prompt (and optional context)\n",
      "2. Example from test dataset by index\n",
      "2025-04-22 00:40:37,342 - __main__ - INFO - Loaded example 1200 from test split.\n",
      "\n",
      "================================================================================\n",
      "Running inference with model: lora_r16\n",
      "Input Prompt:\n",
      "What is the maximum energy efficiency rating of hydroelectric dams in Canada?\n",
      "Input Context:\n",
      "CREATE TABLE hydro_dams (id INT, name TEXT, country TEXT, energy_efficiency_rating FLOAT); INSERT INTO hydro_dams (id, name, country, energy_efficiency_rating) VALUES (1, 'Robert-Bourassa', 'Canada', 0.94), (2, 'Churchill Falls', 'Canada', 0.92);\n",
      "================================================================================\n",
      "2025-04-22 00:40:37,343 - __main__ - INFO - Starting generation...\n",
      "2025-04-22 00:40:37,344 - __main__ - INFO - Using GenerationConfig: GenerationConfig {\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "2025-04-22 00:40:56,181 - __main__ - INFO - Generation finished in 18.84 seconds.\n",
      "\n",
      "--- Raw Model Output (took 18.84s) ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "```\n",
      "\n",
      " প্রবৃত্ত to generate the SQL query.अंतिम\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Extracted SQL ---\n",
      "```sql\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams;\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Reference SQL (Example 1200) ---\n",
      "SELECT MAX(energy_efficiency_rating) FROM hydro_dams WHERE country = 'Canada';\n",
      "--------------------------------------------------------------------------------\n",
      "2025-04-22 00:40:56,181 - __main__ - INFO - Cleaning up...\n",
      "2025-04-22 00:40:56,312 - __main__ - INFO - Inference complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     # Check if running in an interactive environment (like Jupyter)\n",
    "    if 'get_ipython' in globals() or 'google.colab' in sys.modules or os.environ.get(\"IPYKERNEL_CELL_NAME\"):\n",
    "        run_interactive_inference()\n",
    "    else:\n",
    "        logger.info(\"Script appears to be running in a non-interactive environment.\")\n",
    "        logger.info(\"Running interactive inference...\")\n",
    "        run_interactive_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
